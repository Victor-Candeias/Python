{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "# Working with Classification Trees in Python\n",
    "\n",
    "## Learning Objectives\n",
    "Decision Trees are one of the most popular approaches to supervised machine learning. Decison Trees use an inverted tree-like structure to model the relationship between independent variables and a dependent variable. A tree with a categorical dependent variable is known as a **Classification Tree**. By the end of this tutorial, you will have learned:\n",
    "\n",
    "+ How to import, explore and prepare data\n",
    "+ How to build a Classification Tree model\n",
    "+ How to visualize the structure of a Classification Tree\n",
    "+ How to Prune a Classification Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "## 1. Collect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba8413",
   "metadata": {},
   "source": [
    "<p style=\"color:yellow\">In this exercise, we'll use a sample loans data set to build a classification tree that predicts whether a borrower will default or not default on a new loan.\n",
    "\n",
    "We start by importing the Pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # import pandas package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de1a78",
   "metadata": {},
   "source": [
    "Then we import the data into a data frame called loan and preview it to make sure that the input worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan = pd.read_csv(\"loan.csv\")\n",
    "loan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6095cae",
   "metadata": {},
   "source": [
    "## 2. Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2333ce",
   "metadata": {},
   "source": [
    "Now that we have our data, let's try to understand it. First, we get a concise summary of the structure of the data by calling the info method of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976a576",
   "metadata": {},
   "source": [
    "From the summary, we can tell that there are 30 instances in the dataset by looking at the range index. We can also tell that there are three features in the dataset. Looking at the D type column of the summary, we see that the income and loan amount columns hold integer values while the default column holds text, AKA object.\n",
    "Next, we get summary statistics for the data by calling the described method of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc7997",
   "metadata": {},
   "source": [
    "From the statistics, we see that the minimum income value in the data is five. While the maximum value is 34. Note that these values are in the thousands. So what we're seeing here is $5,000 and $34,000. Likewise, the average loan amount is $51,967."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19efce",
   "metadata": {},
   "source": [
    "Next, let's also visually explore the data by creating a few plots.\n",
    "\n",
    "To ensure that our plots show up in line, we run the map plot lib in line command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74fa1c",
   "metadata": {},
   "source": [
    "Then, we import two packages, mapplotlib pyplot and the seaborne package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2f538",
   "metadata": {},
   "source": [
    "The first plot we create is a boxplot. This is a plot to show the difference in annual income between those that did not default, which is no, and those that did default, which is yes. \n",
    "The plot shows that those that did not default on their loans tend to have a higher annual income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data = loan, x = 'Default', y = 'Income')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3262311",
   "metadata": {},
   "source": [
    "Next, let's create another boxplot to show the difference in loan amount between those that did not default on their loans and those that did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c0cf0",
   "metadata": {},
   "source": [
    "This chart shows that those that defaulted on their loans tend to have borrowed a little slightly more than those that did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data = loan, x = 'Default', y = 'Loan Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea181d96",
   "metadata": {},
   "source": [
    "Finally, let's create a scatter plot to look at the relationship between income and loan amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62df07e",
   "metadata": {},
   "source": [
    "This chart doesn't show a clear linear relationship between those two variables. There isn't much we can really infer from it, so we can move on now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(data = loan, \n",
    "                     x = 'Loan Amount', \n",
    "                     y = 'Income', \n",
    "                     hue = 'Default', \n",
    "                     style = 'Default', \n",
    "                     markers = ['^','o'], \n",
    "                     s = 150)\n",
    "ax = plt.legend(bbox_to_anchor = (1.02, 1), loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d214f4",
   "metadata": {},
   "source": [
    "## 3. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef2bf6",
   "metadata": {},
   "source": [
    "Before we build our classification tree though, we need to split the data into training and test sets. Prior to doing so, we must first separate the dependent variable from the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb473eaf",
   "metadata": {},
   "source": [
    "Let's start by creating a data frame called Y for the dependent variable, which is default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570e61b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = loan[['Default']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd5f1e",
   "metadata": {},
   "source": [
    "Then we also do the same and create a second data frame called X for the independent variables, income and loan amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda732e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = loan[['Income', 'Loan Amount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821833a1",
   "metadata": {},
   "source": [
    "Now that we have our two data frames, we need to now build our model. We can now split our data, before we do so, we have to import the train test split function from the SK learn model selections package. Using this, we can split the data, the X and Y data frames, into X_train X_test, Y_train and Y_test. Note that here we set train size to 0.8. This means we want 80% of the original data to become the training data, while 20% becomes the test data. We also set stratify as y which means we want the data splits using stratified random sampling based on the values of y. Finally, we set random state to 1234, simply so we get the same results every time we do the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cdb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size = 0.8,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 1234) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b04f6",
   "metadata": {},
   "source": [
    "Now that our data is split, the shape attribute of the X_train and X_test data frames tell us how many instances, or records, are in each data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb993b57",
   "metadata": {},
   "source": [
    "We can see that we have 24 instances in the training set and six instances in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66162c9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67d2b0",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate the Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73253dfe",
   "metadata": {},
   "source": [
    "To build a classification tree in Python, we need to import the decision tree classifier class from the SK learn tree sub package. We then instant sheet an object from the class. We call the object classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263345cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e440e",
   "metadata": {},
   "source": [
    "Now that we have an object, we can build or fit a classification tree model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184b8f8",
   "metadata": {},
   "source": [
    "To evaluate and estimate the future performance of our model, we can now see how this model fits against the test data. To do so, we pass the test data to the score method of the model. This returns the accuracy of the model against the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfea5e9",
   "metadata": {},
   "source": [
    "A classification tree is only able to accurately explain 50% of the relationship between the independent variables and a dependent variable within the test data. That's no better than a coin toss. We can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa1a33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b2a5e",
   "metadata": {},
   "source": [
    "## 5. Visualize the Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726533ba",
   "metadata": {},
   "source": [
    "Now that we've trained a classification tree, let's visualize it to get a better understanding of the tree logic.\n",
    "\n",
    "First, we make sure that we import the tree object from the sklearn package.\n",
    "The figure_method of Pyplot allows us to specify the size of our tree. Feel free to adjust this to see how it impacts the size of your tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce115cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15ebaa",
   "metadata": {},
   "source": [
    "Finally, we use the plot_tree method of the tree object to visualize the tree.\n",
    "\n",
    "The first argument we pass to this method is the classification tree model itself, model. Then we specify the independent variables as a list. Next, we specify the possible values of the dependent variable as a list in ascending order, No and Yes. Finally, we specify that we want the nodes of the Tree color filled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ecd4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "tree.plot_tree(model, \n",
    "                   feature_names = list(X.columns), \n",
    "                   class_names = ['No','Yes'],\n",
    "                   filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a6419",
   "metadata": {},
   "source": [
    "Now we have our tree. \n",
    "\n",
    "Let's take some time to understand the structure of this classification tree. We see that the root node asks the question is income less than or equal to $14,500? \n",
    "This means that the first splits that the classifier made during the recursive partitioning process is that income equal to 14.5. \n",
    "The fact that income variable was used as the first split, let's us know that it is the most important variable within the dataset in predicting the outcome. \n",
    "The branch to the left of each node is for the Yes response, while the branch to the right is for the No response. \n",
    "\n",
    "Within each node, we get a value for the Gini impurity score. \n",
    "\n",
    "Gini is a measure of the degree of impurity in the partition. The smaller this value is, the more homogenous the items in a partition are. \n",
    "\n",
    "We also see the number of items or samples within each partition. Notice that this value decreases as we work our way down the tree towards the leaf nodes. This is expected since the primary objective of recursive partitioning is to create smaller, more homogenous subsets of the data.\n",
    "\n",
    "The next information in each node, value, indicates the count of items within each class. This is the item distribution. \n",
    "For example, in the root node there are 14 items with a value of No and 10 with a value of Yes. The Noes are the majority, which is why the class value is equal to No. \n",
    "This means that if our classification tree were just one node, the root node, it would label every loan as not default. Notice how the Gini impurity values change in relation to the item distributions. As one class dominates, the Gini value tends toward zero. \n",
    "One of the benefits of decision trees is that they are pretty good at ranking the effectiveness of independent variables and predicting the values of the dependent variable. This is known as feature_importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7a0d8",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c340aa3",
   "metadata": {},
   "source": [
    "We can visualize the feature_importance of the independent variables as follows. \n",
    "First, we assign the feature importances on the score attribute of the model to a variable, which we call Importance. \n",
    "The attribute returns an array of the important scores of each independent variable. Next, we create a Pandas Series called feature_importance by using the importance array as the values and the independent variable names as the index. \n",
    "Finally, we plot the series. Let's take a look at it. \n",
    "\n",
    "From the plot, we see that the income variable is more important than the loan amount in predicting whether a borrower will default on their loan or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbd365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importance = model.feature_importances_\n",
    "feature_importance = pd.Series(importance, index = X.columns)\n",
    "feature_importance.plot(kind = 'bar')\n",
    "plt.ylabel('Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f8ee5",
   "metadata": {},
   "source": [
    "## 6. Prune the Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349612b",
   "metadata": {},
   "source": [
    "Now that we've trained and visualized a classification tree, let's look into what we can do to improve its performance by pruning. Decision trees are prone to overfitting. One telltale sign that a tree has overfit is if it has a high accuracy score on the training data with a low accuracy score on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8405c0",
   "metadata": {},
   "source": [
    "Let's start by getting our trees accuracy on the training data. To do this we pass the training data to the score method of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82aa14",
   "metadata": {},
   "source": [
    "A model is a hundred percent accurate on a training data. That's suspicious. Let's get a second opinion from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522c7ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d2882",
   "metadata": {},
   "source": [
    "Similarly, we pass the test data to the score method of the model. Our model is 50% accurate on the test data. Our model has definitely overfit on the training data and needs to be pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a48afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35536d86",
   "metadata": {},
   "source": [
    "There are two ways to prune a decision tree. One is to set parameters that manage its growth during the recursive partitioning process. This is known as pre-pruning. Another approach is to allow the tree to fully grow on impeded and then gradually reduce its size in order to improve its performance. This is known as post-pruning. In this tutorial, we will use a pre-running approach. This means that we need to figure out the best combination of values for the parameters of the tree that will result in the best performance. This is known as hyper parameter tuning. The psyche learned package scikit-learn provides several parameters we can tune during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8db84",
   "metadata": {},
   "source": [
    "We will limit ourselves to three of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0113a5b6",
   "metadata": {},
   "source": [
    "We start by creating a dictionary which we call grid that holds the values of the parameters we want to try out. \n",
    "\n",
    "The first parameter is max depth. This sets the maximum depth of the decision tree. We will try setting the value to two, three, four and five to see which is the best. \n",
    "\n",
    "The next parameter is min sample split. This sets the minimum number of items we can have in the partition before it can be split. Studies show that a value between one and 40 is best. We will try setting the value to two, three, and four. \n",
    "\n",
    "Next is the min samples leaf parameter. This sets the minimum number of items we have in a leaf node. Studies show that the best values are between one and 20. We will try setting the value to one, two, three, four, five, and six. We set the range from 1 to 7 (the last value isn't use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d647e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'max_depth': [2, 3, 4, 5],\n",
    "         'min_samples_split': [2, 3, 4],\n",
    "         'min_samples_leaf': range(1, 7)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53005f6b",
   "metadata": {},
   "source": [
    "The gridsearch CV class from the scikit-learn model selection sub package allows us to perform a great search to find the best parameter values for our tree. We import the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8774d",
   "metadata": {},
   "source": [
    "Then we instantiate a decision tree classifier object and then we pass the object to a new grid search CV object, which we call GCV. We also pass the parameter grid to the object. We then pass the training data to the fit method of GCV so it evaluates each hyper parameter combination in grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state = 1234)\n",
    "gcv = GridSearchCV(estimator = classifier, param_grid = grid)\n",
    "gcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20281e1",
   "metadata": {},
   "source": [
    "The best estimator attributes of GCV returns the classifier with the best combination of hyper parameters for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fa43c",
   "metadata": {},
   "source": [
    "We then fit a classification tree on the training data using this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd832e61",
   "metadata": {},
   "source": [
    "The output shows that the best combination of hyper parameters is max depth set at two and min samples leaf set at six."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aaf2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = gcv.best_estimator_\n",
    "model_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0a823",
   "metadata": {},
   "source": [
    "Now we can reevaluate how well our model fits the training data by passing the training data to the score method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242053a5",
   "metadata": {},
   "source": [
    "We see that the accuracy has gone down from a hundred percent to 87.5%.\n",
    "\n",
    "Let's see how the model fits the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81f6f8",
   "metadata": {},
   "source": [
    "Now, the model's accuracy on the test data has risen from 50% to 83.3% that is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f0e1c",
   "metadata": {},
   "source": [
    "Finally, we can visualize our prune model. Our prune tree is much smaller than the one we started off with but it generalizes much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "tree.plot_tree(model_, \n",
    "                   feature_names = list(X.columns), \n",
    "                   class_names = ['No','Yes'],\n",
    "                   filled = True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
